{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ea89ea-4bc6-4b7a-8bda-e6af561c0304",
   "metadata": {},
   "source": [
    "---\n",
    "# 딥러닝\n",
    "\n",
    "**Y = 가중치 * X + 바이어스**<Br>\n",
    "모델 학습의 목표는 이 손실(loss) 값을 최소화하는 것 => 손실 값이 작을수록 모델의 예측이 실제값과 더 가깝다는 의미\n",
    "\n",
    "    단위 - 뉴런\n",
    "    입력층 == 특성(Feature)의 개수\n",
    "    출력층 == 출력 값의 개수\n",
    "    각각의 뉴런과 가중치, 편향을 최적화 하기 위함\n",
    "\n",
    "    Label => One-hot Encoding => 데이터의 연관성 제거\n",
    "    Data => to_numpy() or values\n",
    "    Loss function + Activation Function\n",
    "\n",
    "\n",
    "    초기값은 랜덤\n",
    "    Forward -> Loss -> Backward -> Forward -> Loss -> Backword -> ...\n",
    "    \n",
    "    Forward propagation\n",
    "        입력 ( 데이터의 Feature = 특성값 ) -> 각 입력에 대해서 가중치 추출 -> 바이어스와 가중치 ->  활성화 함수(f(x)) -> 출력값 계산 ( 예측된 값 )\n",
    "    \n",
    "    Backward propagation\n",
    "        가중치와 바이어스를 조정(Update)하여 출력값이 실제값에 가깝게 만듦\n",
    "    \n",
    "    Loss funtion\n",
    "        예측값과 실제값를 비교하여 오차를 계산\n",
    "    \n",
    "    Optimizer\n",
    "        가중치와 바이어스 조정 (Update)\n",
    "        업데이트 과정을 결정하는 알고리즘\n",
    "        학습률 (Learning Rate)\n",
    "            - 가중치와 바이어스를 한 번 업데이트할 때 얼마나 크게 변경할지를 결정\n",
    "        목표 \n",
    "            - 손실 함수 값을 최소화\n",
    "            - 가중치와 바이어스를 효율적으로 업데이트하여 모델이 실제값을 더 잘 예측하도록 학습시키는 것\n",
    "        종류\n",
    "            - SGD (Stochastic Gradient Descent) : 기본적인 경사 하강법 알고리즘\n",
    "            - Adam : 학습률을 자동으로 조절하여 효율적인 학습 가능\n",
    "\n",
    "    Activation function\n",
    "        비선형 함수\n",
    "        선형 모델만으로는 학습할 수 없는 복잡한 데이터 패턴을 학습 가능\n",
    "        종류\n",
    "            - ReLU (Rectified Linear Unit) : 입력이 0보다 크면 입력을 그대로 출력하고, 0 이하이면 0을 출력\n",
    "            - Sigmoid : 출력을 0과 1 사이로 압축\n",
    "            - Tanh (Hyperbolic Tangent) : 출력을 -1과 1 사이로 압축\n",
    "            - Softmax : 각 요소는 0과 1 사이의 값을 가지며, 모든 요소의 합은 1이 됩니다. 이는 각 클래스가 정답일 확률의 분포를 나타냄\n",
    "\n",
    "    Accuracy를 높이는 법\n",
    "        Dense(뉴런 개수) 뉴런 개수를 늘린다\n",
    "        model.add(Dense()) == 레이어  => 레이어의 개수를 늘린다\n",
    "\n",
    "    회귀\n",
    "        Loss = mse\n",
    "        Activation = none\n",
    "\n",
    "    분류\n",
    "        이중\n",
    "            Loss = sigmoid\n",
    "            Activatioin = binary_crossentropy\n",
    "        다중\n",
    "            Loss = softmax\n",
    "            Activation = categorical_crossentropy\n",
    "\n",
    "\n",
    "    input_shape(입력 특성의 개수, )\n",
    "\n",
    "경사하강법\n",
    "- Loss Function의 값의 최솟값(가중치)을 찾는 최적화 알고리즘\n",
    "- Current - (Learning Rate) * (Loss Function 미분) => Next\n",
    "- 기울기를 미분하면 현재 위치가 나옴\n",
    "- 양수  -  그래프의 오른쪽  -  값을 줄여야 함\n",
    "- 음수  -  그래프의 왼쪽    -  값을 키워야 함\n",
    "\n",
    "      - 배치 경사하강법\n",
    "          전체 학습 데이터셋의 모든 샘플에 대한 손실을 계산하고, 평균 기울기 사용\n",
    "          안정적, 속도 느림\n",
    "      - 확률적 경사하강법 ( Stochastic Gradient Descent, SGD )\n",
    "          하나의 무작위로 선택된 데이터 샘플에 대한 손실을 계산하고, 그 기울기를 사용\n",
    "          속도 빠름, 노이즈가 많고 불안정\n",
    "      - 미니배치 경사하강법\n",
    "          소일정 크기의 무작위로 선택된 데이터 샘플들의 묶음 (미니배치) 에 대한 손실을 계산하고, 평균 기울기 사용\n",
    "          균형잡힌 성능"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
